{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ea3b94",
   "metadata": {},
   "source": [
    "### **Capstone Project: \"Versatile PDF Chatbot: Conversational RAG with Dynamic LLM Backends\"**\n",
    "\n",
    "**Project Goal:** Develop an end-to-end, interactive web-based chatbot that allows users to upload PDF documents, engage in a natural language conversation about the document's content, and dynamically switch between different Large Language Model (LLM) inference backends (local, fast cloud API, and potentially a general cloud API). The chatbot will maintain chat history for coherent dialogue.\n",
    "\n",
    "\n",
    "**1. PDF Ingestion & RAG Pipeline:**\n",
    "\n",
    "***Document Upload & Loading:*** Implement a Streamlit UI component for users to upload PDF files. Use PyPDFLoader (or similar LangChain loader) to load the content.\n",
    "\n",
    "***Text Splitting:*** Apply RecursiveCharacterTextSplitter to break down the PDF content into manageable chunks suitable for embedding and retrieval.\n",
    "\n",
    "***Embeddings:*** Generate text embeddings for the document chunks. Students should use a local HuggingFace embedding model (e.g., sentence-transformers/all-MiniLM-L6-v2) to avoid external API dependencies for embeddings, making the RAG core self-contained.\n",
    "\n",
    "***Vector Store:*** Set up and populate a local Vector Store (e.g., FAISS or ChromaDB) with the document chunks and their embeddings. This vector store will serve as the knowledge base for RAG.\n",
    "\n",
    "***Retriever:*** Configure a suitable retriever (e.g., VectorStoreRetriever) to fetch relevant document chunks based on user queries.\n",
    "\n",
    "**2. Conversational Core with Message History:**\n",
    "\n",
    "***Chat History Management:*** Implement LangChain's memory components (e.g., ConversationBufferMemory, ConversationBufferWindowMemory) to store and manage the ongoing conversation history.\n",
    "\n",
    "***Prompt Templates:*** Design a dynamic prompt template that incorporates the chat history, the user's current question, and the retrieved context from the RAG system. This template will guide the LLM's responses in a conversational manner.\n",
    "\n",
    "***Conversational Chain:*** Construct a LangChain conversational chain that combines the RAG retriever with the LLM and the message history.\n",
    "\n",
    "**3. Dynamic LLM Backend Integration:**\n",
    "\n",
    "***Ollama Integration:*** Implement a connection to a locally running Ollama instance. Users should be able to select an Ollama-served model (e.g., Llama2, Mistral, Gemma) as their LLM backend.\n",
    "\n",
    "***Groq API Integration:*** Integrate the Groq API (specifically using Llama3) as another selectable LLM backend. This will highlight the LPU's speed.\n",
    "\n",
    "***LLM Switching Logic:*** Implement Streamlit UI elements (e.g., a dropdown or radio buttons) that allow the user to seamlessly switch between the configured LLM backends during a chat session.\n",
    "\n",
    "**4. Streamlit Web Application:**\n",
    "\n",
    "***User Interface:*** Create an intuitive Streamlit interface featuring:\n",
    "\n",
    "A clear section for PDF upload.\n",
    "\n",
    "A chat window displaying the conversation history.\n",
    "\n",
    "An input field for user questions.\n",
    "\n",
    "Buttons/dropdowns for selecting the active LLM backend.\n",
    "\n",
    "Loading indicators while the LLM generates responses.\n",
    "\n",
    "***Session State Management:*** Effectively use Streamlit's st.session_state to persist chat history, uploaded document data, and selected LLM configurations across user interactions.\n",
    "\n",
    "**Expected Deliverables:**\n",
    "\n",
    "Runnable Python Codebase: A well-structured, modular, and extensively commented Python project containing all components (Streamlit app, LangChain RAG pipeline, LLM integrations).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
