# **Project: Building a Versatile PDF Chatbot Using Conversational RAG with Dynamic LLM Backends**

**Project Goal:** Develop an end-to-end, interactive web-based chatbot that allows users to upload PDF documents, engage in a natural language conversation about the document's content, and dynamically switch between different Large Language Model (LLM) inference backends (local, fast cloud API, and a general cloud API). The chatbot will maintain chat history for coherent dialogue.

## Streamlit Web-Based Chatbot for PDF Analysis

This project primarily focuses on building a web-based chatbot using Streamlit that is focused on analyzing and asking questions about the contents of any PDF file that the user uploads, while engaging in a natural back and forth conversation.

A modular `Retrieval-Augmented Generation (RAG)` pipeline was implemented using LangChain to answer questions about whatever PDF file the user uploads, by retrieving relevant content based on the user query and passing it to the LLM along with the prompt as additional context.

When it comes to the web application, the user has the option to dynamically choose between different LLMs, specifically a local `Ollama`-based model, a general cloud API model using `Google Gemini`, and a fast cloud API model via `Groq` (Llama-3). In addition, the user also has the ability to see the entire chat history over the duration of the session, as well as see what is happening behind the scenes as a response is being generated by the LLM of choice, through different loading indicators.

## Folder Structure

This project includes the following:

### `screenshots/`

Contains screenshots of the Streamlit UI created for the chatbot, where a series of questions pertaining to a specific PDF article about AI diversity and inclusion were asked. The screenshots show the AI responses for different questions being asked using different LLMs as the backend. In addition, the overall chat history of all of the previous questions and answers can also be seen, by toggling on/off a switch called *"Show full chat history"* in the UI.

### `sample_documents/`

Contains two sample PDF files that I used for testing the Streamlit web application and the overall chatbot interaction in terms of asking various questions about the document's content. That being said, these are optional, and the user is free to choose their own PDF files for asking questions and interacting with the chatbot.

### `scripts/`

Contains Python scripts to build and use the RAG system built for context retrieval for question-answering purposes. The overall logic is broken into several scripts for modularity, and includes files for setting configurations, file processing, utility functions, building the RAG system, and a main file called `app.py` for running the entire pipeline, from loading the Ollama/Gemini/Groq LLM models and document chunking/embedding, to creating different prompt templates for the LLM to use, to building the RAG pipeline, to finally allowing the user to ask specific questions based on the collected data used and get answers, all within the Streamlit web application.

**To run the overall pipeline:**

- Run `streamlit run scripts/app.py` in the terminal. 
- This will open the Streamlit web application in a new browser tab, where the user is prompted to upload a PDF file of their choice. In addition, the user will also have the option to choose a specific LLM to use for question answering (default is Ollama)
- To see some examples of what the Streamlit UI looks like without running the code, refer to the `screenshots` folder mentioned earlier.

**Disclaimer**: The local RAG system that was built, while effective with question answering tasks, is not really well-suited for summarizing large bodies of text or entire documents/files. As a result, trying to summarize an entire file or large body of text may yield unexpected or poor results.

### `requirements.txt`

Lists all required Python packages. Install with `pip install -r requirements.txt`.

---

## Configuration

When running this code on your own device, `config.py` can be modified as follows:

- Set paths for `DOCUMENTS_FOLDER`, `VECTOR_DB_DIR`, etc.
- Set API keys and other environment variables.
- Configure `chunk_size`, `chunk_overlap`.
    - For this project, I specifically chose a chunk size of 800 and an overlap of 200. In a previous project, I only used a chunk size of 600 due to issues with memory issues at times with larger chunk sizes. That being said, I was able to increase the chunk size to 800 this time consistently without running into any memory issues. This increased chunk size allows for more context per retrieved document, while also not being so large that memory becomes an issue. As for the overlap, I felt that it allowed for enough context to be preserved between chunks, without there being too much repitition. As for the Groq and Gemini LLMs, these chunk size and overlap parameters could be increased, due to API calls being used as opposed to loading an entire model into memory.
- Define instruction templates and base prompt.

## Additional Notes:

The links to the PDF documents used for testing the chatbot for question-answering can be found below:

https://www.jair.org/index.php/jair/article/view/17806

https://www.jair.org/index.php/jair/article/view/15315

